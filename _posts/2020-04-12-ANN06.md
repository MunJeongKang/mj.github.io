---
title: "인공신경망 - Batch and Minibatch Algorithms"
author: Munjeong Kang
date: 2020-04-12 23:00:08 +0800
categories: [class, ANN]
tags: [ANN]
toc: true
comments: true
---

-----

<div style = "font-weight:500; font-size:1.0em; margin-left: 1em; margin-right: 1em;text-align:justify; ">
머신러닝을 위한 최적화 알고리즘은 일반적으로 전체 비용 함수의 <b style = "color:#d7385e;font-size:1.2">일부</b>만 사용하여 추정된 비용 함수의 기대값에 기초하여 파라미터에 대한 각각의 업데이트를 계산한다. 최적화 알고리즘의 대부분에서 사용되는 목적함수 $J$의 성질도 트레이닝셋에 대한 기대(expextations)이다. 가장 일반적으로 사용되는 성질은 gradient이다. 
$$
\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{x}, y \sim \hat{p}_{\mathrm{data}}} \nabla_{\boldsymbol{\theta}} \log p_{\mathrm{model}}(\boldsymbol{x}, y ; \boldsymbol{\theta})
$$
실제로 데이터셋에서 작은 수의 예제를 랜덤하게 샘플링하고 그 예제에 대한 평균을 구해 이러한 기대치를 계산할 수 있다. 대부분의 최적화 알고리즘은 정확한 기울기를 천천히 계산하는 것보다 기울기의 근사된 추정치를 빠르게 계산할 수 있는 경우가 총 계산 관점에서 <b style = "color:#d7385e;font-size:1.2">훨씬 더 빨리 수렴(converge)</b>한다. 작은 샘플에서 기울기의 통계적인 추청에 동기를 부여하는 또 다른 고려사항은 트레이닝셋의 <b style = "color:#d7385e;font-size:1.2">불필요한 중복(redundancy)</b>이다. 기울기에 매우 유사한 기여를 하는 많은 예시를 찾을 수 있을 것이다. 
<br><br>
전체 트레이닝셋을 사용하는 최적화 알고리즘을 <b style = "color:#d7385e;font-size:1.2">batch </b>또는 <b style = "color:#d7385e;font-size:1.2">deterministic gradient method</b>라 부른다. 한번에 하나의 예시만 사용하는 최적화 알고리즘은 <b style = "color:#d7385e;font-size:1.2">stochastic</b> 또는 <b style = "color:#d7385e;font-size:1.2">online methods</b>라 부른다. online이라는 용어는 주로 연속적으로 생성된 예제에서 추출할 때 사용된다. 딥러닝에 사용되는 알고리즘은 이 사이 어딘가 있지만, 모든 트레이닝 예제보다는 적게 사용한다. 이를 전통적으로 <b style = "color:#d7385e;font-size:1.2">minibatch</b> 또는 <b style = "color:#d7385e;font-size:1.2">minibatch stochastic methods</b>라 부른다. 이제는 simply stochastic methods로 흔히 부른다. 
<br><br>
미니배치 크기는 일반적으로 다음과 같은 요인에 의해 나타난다.
<ul>
<li>배치 크기가 클수록 기울기에 대한 정확한 추정치를 제공하지만 linear returns 보단 아니다.</li>
<li>Multicore architectures는 극도로 작은 배치에 의해 주로 이용되지 않는다. </li>
<li>만약 배치의 모든 예시를 동시에(in parallel) 처리하려면 배치 사이즈에 따라 메모리 양을 조정해야한다.</li>
<li>어떤 하드웨어는 특정한 크기의 array로 더 나은 실행시간(runtime)을 얻는다.</li>
<li>작은 배치는 정규화 효과를 제공할 수 있다.</li>
</ul>
기울기 $\boldsymbol{g}$에만 기초하여 업데이트를 계산하는 방법은 주로 상대적으로 robust하고 100과 같이 더 작은 배치 크기를 처리할 수 있다. 또한, 헤시안 행렬 $\boldsymbol{H}$와 $\boldsymbol{H}^{-1}\boldsymbol{g}$와 같은 계산 업데이트를 사용하는 second-order methods는 일반적으로 10,000과 같은 훨씬 큰 배치 사이즈가 요구된다. 
<br><br>
샘플 집합에서 기대되는 기울기의 unbiased estimate를 계산하려면 샘플이 독립이어야 한다. 또한, 두 개의 subsequent 미니배치 예제도 서로 독립이어야 한다. 많은 데이터셋은 <b style = "color:#d7385e;font-size:1.2">연속적인(successive) 예시들이 높은 상관관계를 갖는 방식</b>으로 가장 자연스럽게 배열된다. 이러한 경우, 데이터셋의 순서가 어느 정도 유의하면 미니배치를 선택하기 전에 예시를 섞을 필요가 있다. 
<br><br>
minibatch stochastic gradient descent의 흥미로운 동기는 어떤 예제가 반복되지 않는 한 true 일반화 에러의 기울기를 따른다는 것이다. stochastic gradient descent가 일반화 오류를 최소화하는 사실은 <b style = "color:#d7385e;font-size:1.2">online learning</b>에서 가장 쉽게 볼 수 있으며 예시나 미니배치는 데이터 흐름으로 부터 나온다. 이 시나리오에서 예시는 절대 반복되지 않는다. 모든 experience는 $p_{\text{data}}$의 공정한 샘플이다.
<br><br>