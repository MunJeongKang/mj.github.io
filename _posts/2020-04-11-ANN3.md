---
title: "인공신경망 - Output Units"
author: Munjeong Kang
date: 2020-04-11 23:00:08 +0800
categories: [class, ANN]
tags: [ANN]
toc: true
comments: true
---
-----

<div style = "font-weight:500; font-size:1.0em; margin-left: 1em; margin-right: 1em;text-align:justify; ">
비용 함수의 선택은 출력 단위(output unit)의 선택과 밀접하게 연결된다. 대부분의 경우, 단순하게 데이터 분포와 모델 분포 사이의 <b style = "color:#d7385e;font-size:1.2">cross-entropy</b>를 사용한다. 우리는 feedforward 네트워크가 $h=f(x;\theta)$로 정의된 숨겨진 특징(hidden features)을 제공한다고 가정한다. 출력 계층(output layer)의 역할은 네트워크가 수행해야 하는 task를 완료하기 위하여 특징에 추가적인 변환을 제공하는 것이다.
<br><br>
<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Linear Units for Gaussian Output Distributions
</span>
<br><br>
간단한 출력 단위 중 하나는 비선형성(nonlinearity)이 없는 <b style = "color:#d7385e;font-size:1.2">affine 변환</b>에 기초한 것이다. 이것들은 주로 선형 단위로 불리며 특징 $h$가 주어질 때, 선형 출력 단위의 계층은 벡터 $\hat{y}=W^{\top} h+b$를 생성한다. 선형 출력 계층들은 <b style = "color:#d7385e;font-size:1.2">조건부 가우시안 분포</b>인 $p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I})$의 평균을 생성하는데 사용된다. 선형 단위는 <b style = "color:#d7385e;font-size:1.2">포화 상태(saturate)가 아니기</b> 때문에 기울기(gradient) 기반 최적화 알고리즘에는 별다른 어려움이 없으며 다양한 최적화 알고리즘과 함께 사용될 수 있다. 
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Sigmoid Units for Bernoulli Output Distributions
</span>
<br><br>
많은 작업에서 <b style = "color:#d7385e;font-size:1.2">이진 변수</b>(binary variable)인 $y$값을 예측해야 한다. 베르누이 분포는 단 하나의 수로 정의되므로 신경망(neural net)은 $P(y=1|x)$만 예측하면 된다. 이 숫자가 유효한 확률이 되려면 [0,1]간격에 있어야 하고 이 제약을 만족하려면 신중하게 디자인 할 필요가 있다. 유효한 확률을 얻기 위해 선형 단위를 사용하고 그 값을 한계점(threshold)으로 사용한다고 다음과 같이 가정한다.<br>
<p align="center">
<br>
 $P(y=1 | \boldsymbol{x})=\max \left\{0, \min \left\{1, \boldsymbol{w}^{\top} \boldsymbol{h}+b\right\}\right\}$
 </p>
경사하강법(gradient decent)으로는 이를 매우 효과적으로 훈련시킬 수 없다. $w^{\top} h+b$가 단위 구간을 벗어나면 파라미터에 대한 모델의 아웃풋 기울기가 0이 되기 때문이다. 대신 모델이 잘못된 답을 줄 때마다 항상 강한(strong) 기울기를 보장하는 다른 접근법을 사용하는 것이 좋다. <b style = "color:#d7385e;font-size:1.2">시그모이드 출력 단위</b>는 $\hat{y}=\sigma\left(\boldsymbol{w}^{\top} \boldsymbol{h}+b\right)$로 정의되며 $\sigma$는 로지스틱 시그모이드 함수이다. 
<br><br>
시그모이드 출력 단위는 두 개의 구성 요소를 가지고 있다고 생각할 수 있다. 
<ol>
<li>$z=w^{\top} h+b$를 계산하기 위해 <b style = "color:#d7385e;font-size:1.2">선형 레이어</b>를 사용하는 것</li>
<li>$z$를 확률로 변환하기 위해 <b style = "color:#d7385e;font-size:1.2">시그모이드 활성화(activation) 함수</b>를 사용하는 것</li>
</ol>
만약 비정규화된(unnormalized) 로그 확률이 $y$와 $z$에서 선형이라고 가정한다면 비정규화된 확률을 얻기 위해 <b style = "color:#d7385e;font-size:1.2">지수화(exponentiate)</b>할 수 있다. $z$의 시그모이드 변환에 의해 베르누이 분포 산출을 위한 <b style = "color:#d7385e;font-size:1.2">정규화</b> 과정은 다음과 같다. 
<br><br>
<p align="center">
$\begin{aligned} 
\log \tilde{P}(y) &=y z \\ 
\tilde{P}(y) &=\exp (y z) \\ 
P(y) &=\frac{\exp (y z)}{\sum_{y^{\prime}=0}^{1} \exp \left(y^{\prime} z\right)} \\ 
P(y) &=\sigma((2 y-1) z) 
\end{aligned}$
</p>
로그 공간에서 확률을 예측하는 이 접근법은 <b style = "color:#d7385e;font-size:1.2">최대 우도(maximum likelihood)</b> 학습에 사용하는 것이 보통이다. 왜냐하면 최대 우도에 사용되는 비용 함수가 $ \log P(y|x)$이기 때문에, 비용함수에서 로그는 시그모이드의 exp를 없애준다. 이러한 효과가 없다면, 시그모이드의 포화상태는 gradient 기반 학습의 진전을 막았을 것이다. 
<br><br>
시그모이드에 의해 파라미터화되는 베르누이의 최대 우도 학습에 대한 손실 함수(loss function)는 다음과 같다. $\zeta$ 는 softplus 함수이다.
<br><br>
<p align="center">
$ \begin{aligned} 
J(\boldsymbol{\theta}) &=-\log P(y | \boldsymbol{x}) \\ 
&=-\log \sigma((2 y-1) z) \\ 
&=\zeta((1-2 y) z) 
\end{aligned}$
</p>
$(1-2y)z$가 very negative(매우 작은 음수)한 경우만 포화되므로 포화상태는 모델이 이미 올바른 정답을 가지고 있을 때만 발생한다. 즉, $y=1$ 이고 $z$가 very positive(매우 큰 양수) 하거나 $y=0$이고 $z$가 very negative 한 경우만 발생한다. $z$가 잘못된 사인을 가질 때, softplus 함수에 대한 인수(argument) $(1-2y)z$는 $|z|$로 단순화될 수 있다. 
<br><br>
<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Softmax Units for Multinoulli Output Distributions
</span>
<br><br>
$n$개의 값을 가지는 이산 변수의 경우로 일반화하기 위해 $\hat{y}_{i}=P(y=i | \boldsymbol{x})$에 대한 벡터 $\hat{y}$가 필요하다. 먼저 선형 계층이 비정규화된 로그 확률을 다음과 같이 예측한다.
 <p align="center">
<br>
 $z=W^{\top}h+b \quad $ where $z_{i}=\log \tilde{P}(y=i | \boldsymbol{x})$
 </p>
소프트맥스 함수는 원하는 $\hat{y}$를 얻기 위해 $z$를 <b style = "color:#d7385e;font-size:1.2">지수화</b>하고 <b style = "color:#d7385e;font-size:1.2">정규화</b>할 수 있다. 
<p align="center">
<br>
 $\operatorname{softmax}(\boldsymbol{z})_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}$
 </p>
이를 다음과 같이 <b style = "color:#d7385e;font-size:1.2">최대화</b> 하길 원한다. 
<p align="center">
<br>
 $ \begin{aligned} 
 \log P(\mathrm{y}=i ; z)&=\log \operatorname{softmax}(z)_{i} \\
 \log \operatorname{softmax}(\boldsymbol{z})_{i}&=z_{i}-\log \sum_{j} \exp \left(z_{j}\right)
 \end{aligned}
 $
 </p>
 두번째 항은 $\max_j z_j$로 근사될 수 있다. 만약 올바른 답이 소프트맥스에 대한 가장 큰 입력을 이미 가지고 있다면, $-z_i$ 항과 $\log \sum_{j} \exp \left(z_{j}\right) \approx \max _{j} z_{j}=z_{i}$ 항은 거의 취소된다. 이 예제는 전체 트레이닝 비용에는 기여하지 못하지만 아직 정확하게 분류되지 않은 다른 예제에 의해 지배될(dominated) 것이다. 
 <br><br>

신경과학적인(neuroscientific) 관점에서 소프트 맥스를 단위들 사이의 경쟁의 형태로 만든다는 생각은 흥미롭다. 
<b style = "color:#d7385e;font-size:1.2">소프트맥스 아웃풋의 합은 항상 1</b>이기 때문에 한 단위 값이 증가하면 다른 단위 값은 감소한다. 이는 피질(cortex) 안에 있는 근처 뉴런사이에 존재하는 것으로 여겨지는 측면 억제(lateral inhibition)와 유사하다. "소프트(soft)"라는 용어는 소프트맥스가 <b style = "color:#d7385e;font-size:1.2">연속</b>이고 <b style = "color:#d7385e;font-size:1.2">미분가능하다</b>는 사실에서 유래하였으며, 소프트맥스 함수는 argmax의 softened 버전을 제공한다. 
<br><br>

<span style = "font-weight:700; font-size:1.3em;  margin-right: 1em;">
Other Output Types
</span>
<br><br>
신경망(neural network)은 우리가 원하는 거의 모든 종류의 출력계층을 일반화할 수 있다. <b style = "color:#d7385e;font-size:1.2">최대 우도 원리(principle)</b>는 출력 계층에 대해 비용 함수를 좋게 설계하는 방법에 대한 가이드를 준다. 조건부 분포 $p(y|x;\theta)$를 정의한다면 최대우도원리는 비용 함수를 $\log p(y|x;\theta)$로 사용하도록 한다. 
<br><br>
일반적으로 신경망은 함수 $f(x;\theta)$를 표현하는 것으로 생각할 수 있다. 이 함수의 아웃풋은 $y$ 값을 직접 예측하는 것은 아니다. 대신 $f(x;\theta)=\omega$가 $y$ 에 대한 분포의 파라미터를 제공한다. 손실함수는 $-\log p(y;\omega(x))$로 해석될 수 있다. 
<br><br>
우리는 가끔 multimodal regression을 수행할 수도 있다. 이는 동일한 $x$값에 대해 y 스페이스에 여러개의 다른 peak들을 가질 수 있는 조건부 분포 $p(y|x)$에서 실제 값을 예측하는 것이다. 이 경우에는 <b style = "color:#d7385e;font-size:1.2">Gaussian mixture</b>를 아웃풋으로 하고 이런 신경망을 흔히 mixture density networks라고 한다. 
<p align="center">
$$
p(\boldsymbol{y} | \boldsymbol{x})=\sum_{i=1}^{n} p(\mathrm{c}=i | \boldsymbol{x}) \mathcal{N}\left(\boldsymbol{y} ; \boldsymbol{\mu}^{(i)}(\boldsymbol{x}), \boldsymbol{\Sigma}^{(i)}(\boldsymbol{x})\right)$$
</p>
그러나 조건부 가우시안 혼합의 gradient 기반 최적화는 신뢰할 수 없다. 왜냐하면 수치적으로 불안정한 분산에 의한 division을 얻을 수 있기 때문이다. 특정한 예시에서 일부 분산이 작은 경우 매우 큰 gradient를 산출한다. 가우시안 혼합 아웃풋은 특히 물리적인 객체의 <b style = "color:#d7385e;font-size:1.2">음성</b>과 <b style = "color:#d7385e;font-size:1.2">움직임</b>의 생성 모델에 효과적이다. 아래 그림은 mixture density의 아웃풋이다. 
<br><br>

<p align="center">
<img src="/images/post_img/AN6.png" width="650" height="300">
</p>
